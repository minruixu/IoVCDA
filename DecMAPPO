import argparse
import os
import pprint
from turtle import shape
from webbrowser import get
import tqdm
import gym
import time
import numpy as np
import torch
from torch.distributions import Independent, Normal
from torch.utils.tensorboard import SummaryWriter
from typing import Any, Callable, DefaultDict, Dict, Optional, Tuple, Union
from tianshou.data import (
    Batch,
    CachedReplayBuffer,
    ReplayBuffer,
    ReplayBufferManager,
    VectorReplayBuffer,
    to_numpy,
)
from tianshou.env import DummyVectorEnv
from tianshou.policy import BasePolicy,PPOPolicy
from tianshou.trainer import onpolicy_trainer
from tianshou.utils import TensorboardLogger, tqdm_config
from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net
from tianshou.utils.net.discrete import Actor
import tianshou.utils.net.discrete as discrete
from tianshou.utils.net.continuous import ActorProb
import tianshou.utils.net.continuous as continuous
from IoVCDA import IoVCDA
import matplotlib.pyplot as plt

# os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"
def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--reward-threshold', type=float, default=None)
    parser.add_argument('--seed', type=int, default=1)

    parser.add_argument('--buffer-size', type=int, default=20000)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--gamma', type=float, default=0.95)
    
    # parser.add_argument('--n-agents', type=int, default = 30, help="Number of agents in the env")
    
    parser.add_argument('--epoch', type=int, default=500)
    parser.add_argument('--step-per-epoch', type=int, default=2000)
    parser.add_argument('--episode-per-collect', type=int, default=16)
    parser.add_argument('--repeat-per-collect', type=int, default=2)

    parser.add_argument('--batch-size', type=int, default=128)
    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[256, 256])
    
    parser.add_argument('--training-num', type=int, default=16)
    parser.add_argument('--test-num', type=int, default=10)
    
    parser.add_argument('--logdir', type=str, default='log')
    parser.add_argument('--render', type=float, default=0.)
    parser.add_argument(
        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'
    )
    # ppo special
    parser.add_argument('--vf-coef', type=float, default=0.25)
    parser.add_argument('--ent-coef', type=float, default=0.0)
    parser.add_argument('--eps-clip', type=float, default=0.2)
    parser.add_argument('--max-grad-norm', type=float, default=0.5)
    parser.add_argument('--gae-lambda', type=float, default=0.95)
    parser.add_argument('--rew-norm', type=int, default=1)
    parser.add_argument('--dual-clip', type=float, default=None)
    parser.add_argument('--value-clip', type=int, default=1)
    parser.add_argument('--norm-adv', type=int, default=1)
    parser.add_argument('--recompute-adv', type=int, default=0)
    parser.add_argument('--resume', action="store_true")
    parser.add_argument("--save-interval", type=int, default=4)
    # env settings
    parser.add_argument("--user-n", type=int, default=10)
    parser.add_argument("--server-n", type=int, default=10)
    parser.add_argument('--wl', type=float, default=0.5)
    parser.add_argument('--we', type=float, default=0.5)
    parser.add_argument('--classical', type=float, default=15e9)
    parser.add_argument('--quantum', type=int, default=1000)
    parser.add_argument('--qubit-time', type=float, default=50e-3)
    args = parser.parse_known_args()[0]
    return args

def build_policy(args: argparse.Namespace = get_args()):
# agents 同时存在离散和连续的动作, 需要两个policy, 但是这两个policy可以共享一个net
    env = IoVCDA(args.user_n)
    # discrete policy
    dnet = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)
    if torch.cuda.is_available():
        dactor = DataParallelNet(
            Actor(dnet, args.discrete_action_shape, device=None).to(args.device)
        )
        dcritic = DataParallelNet(discrete.Critic(dnet, device=None).to(args.device))
    else:
        dactor = Actor(dnet, args.discrete_action_shape, device=args.device).to(args.device)
        dcritic = discrete.Critic(dnet, device=args.device).to(args.device)
    dactor_critic = ActorCritic(dactor, dcritic)
    # orthogonal initialization
    for m in dactor_critic.modules():
        if isinstance(m, torch.nn.Linear):
            torch.nn.init.orthogonal_(m.weight)
            torch.nn.init.zeros_(m.bias)
    doptim = torch.optim.Adam(dactor_critic.parameters(), lr=args.lr)
    ddist = torch.distributions.Categorical
    dpolicy = PPOPolicy(
        dactor,
        dcritic,
        doptim,
        ddist,
        discount_factor=args.gamma,
        max_grad_norm=args.max_grad_norm,
        eps_clip=args.eps_clip,
        vf_coef=args.vf_coef,
        ent_coef=args.ent_coef,
        gae_lambda=args.gae_lambda,
        reward_normalization=args.rew_norm,
        dual_clip=args.dual_clip,
        value_clip=args.value_clip,
        action_space=env.action_space,
        deterministic_eval=True,
        advantage_normalization=args.norm_adv,
        recompute_advantage=args.recompute_adv
    )

    return dpolicy

def test_decmappo(args=get_args()):
    env = IoVCDA(args.user_n)
    args.n_agents = env.user_n
    learning_agents = ["agent_" + str(r) for r in range(env.user_n)]
    exploration_noise = False
    args.state_shape = env.observation_space.shape
    args.discrete_action_shape = env.action_space.n
    args.continuous_action_shape = (1,)
    args.max_action = 1
    # if args.reward_threshold is None:
    #     default_reward_threshold = {"CartPole-v0": 195}
    #     args.reward_threshold = default_reward_threshold.get(
    #         args.task, env.spec.reward_threshold
    #     )
    # train_envs = gym.make(args.task)
    # you can also use tianshou.env.SubprocVectorEnv
    train_envs = DummyVectorEnv(
        [lambda: IoVCDA(args.user_n) for _ in range(args.training_num)]
    )
    # test_envs = gym.make(args.task)
    test_envs = DummyVectorEnv(
        [lambda: IoVCDA(args.user_n) for _ in range(args.test_num)]
    )
    # seed
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    train_envs.seed(args.seed)
    test_envs.seed(args.seed)
    # model
    d_policies = {}
    for agent in learning_agents:
        d_p = build_policy(args)
        d_policies[agent] = d_p
    # buffer
    d_buffers = {agent: VectorReplayBuffer(args.buffer_size, len(train_envs)) for agent in learning_agents}
    # 处理好这个的数据结构
    dis_data = {agent: Batch(
            obs={},
            act={},
            rew={},
            terminated={},
            truncated={},
            done={},
            obs_next={},
            info={},
            policy={}
        ) for agent in learning_agents}
    def reset_env():
        rval = train_envs.reset() # envs * agents
        # 每个agent 提取他们自己的obs
        for i in range(args.n_agents):
            dis_data[learning_agents[i]].obs = rval[:,i]

    def _reset_env_with_ids(ids):
        rval = env.reset(ids)
        for i in range(args.n_agents):
            dis_data[learning_agents[i]].obs_next[ids] = rval[i]
    def reset_buffer():
        for agent in learning_agents:
            d_buffers[agent].reset()
    def reward_metric(rews): # ``f(rewards: np.ndarray with shape (num_episode, agent_num)) -> np.ndarray with shape (num_episode,)``
        # 目的是将所有agent的奖励统一起来
        # This function specifies what is the desired metric, e.g., the reward of agent 1 or the average reward over all agents.
        print(rew.shape)
        return rews[:, 0]


    # 自己写trainer
    all_rew = []
    for e in range(args.epoch):
        # reset data, 
        dis_data = {agent: Batch(
            obs={},
            act={},
            rew={},
            terminated={},
            truncated={},
            done={},
            obs_next={},
            info={},
            policy={}
        ) for agent in learning_agents}
        # 第一个state 是在reset得到的
        reset_env()
        reset_buffer()
        collect_step, collect_episode, collect_time = 0, 0, 0.0
        _ready_env_ids = np.arange(args.training_num)
        temp_rew = []
        with tqdm.tqdm(
            total=args.step_per_epoch, desc=f"Epoch #{e}", **tqdm_config
        ) as t:
            while t.n < t.total:
                data: Dict[str, Any] = dict()
                result: Dict[str, Any] = dict()
                # collect data
                ready_env_ids = _ready_env_ids
                last_rew, last_len = 0.0, 0
                start_time = time.time()

                step_count = 0
                episode_count = 0
                episode_rews = []
                episode_lens = []
                episode_start_indices = []
                last_dis_state: Dict[str, Any] = dict()
                d_result: Dict[str, Any] = dict()
                d_policy: Dict[str, Any] = dict()
                d_state: Dict[str, Any] = dict()
                d_act: Dict[str, Any] = dict()
                while True:
                    all_action_remap = np.array([])
                    for agent in learning_agents:
                        # whole_con_data[agent] = con_data[agent]
                        # whole_dis_data[agent] = dis_data[agent]
                        dis_data[agent] = dis_data[agent][ready_env_ids]
                        last_dis_state[agent] = dis_data[agent].policy.pop("hidden_state", None)
                        with torch.no_grad():
                            # print("-", type(d_policies[agent]))
                            d_result[agent] = d_policies[agent](dis_data[agent], last_dis_state[agent])
                        d_policy[agent] = d_result[agent].get("policy", Batch())
                        d_state[agent] = d_result[agent].get("state", None)
                        if d_state[agent] is not None:
                            d_policy[agent].hidden_state = d_state[agent]
                        d_act[agent] = to_numpy(d_result[agent].act)
                        if exploration_noise:
                            d_act[agent] = d_policies[agent].exploration_noise(d_act[agent], dis_data[agent])
                        dis_data[agent].update(policy=d_policy[agent], act=d_act[agent])
                        # 将全部动作合在一起
                        dis_act = d_policies[agent].map_action(dis_data[agent].act).reshape(-1,1,1)
                        if all_action_remap.shape[0] == 0:
                            all_action_remap = np.concatenate((dis_act, con_act), axis=2)
                        else:
                            all_action_remap = np.concatenate((all_action_remap, dis_act),axis=2)
                            all_action_remap = np.concatenate((all_action_remap, con_act),axis=2)
                    # 像环境进行输入, step
                    result = train_envs.step(all_action_remap)
                    # 对收集上来的result进行处理
                    obs_next, rew, done, info = result # 这里出来的还是np数组, 还不是dic
                    # con_data = whole_con_data
                    # dis_data = whole_dis_data
                    temp_rew.append(rew.mean())
                    # print(rew)
                    for i in range(args.n_agents):
                        dis_data[learning_agents[i]].update(
                            obs_next=obs_next[:,i],
                            rew=rew[:,i],
                            done=done[:,i],
                            info=info
                        )
                        # print("+",con_data[learning_agents[i]].act)
                    # add data to the buffer
                    # to do: start from here
                    for agent in learning_agents:
                        ptr, ep_rew, ep_len, ep_idx = d_buffers[agent].add(
                            dis_data[agent], buffer_ids=ready_env_ids
                        )
                    
                    # print(ep_rew)
                    # collect statistics resutls
                    step_count += len(ready_env_ids)
                    if np.any(done):
                        env_ind_local = np.where(done)[0]
                        episode_count += len(env_ind_local)
                        episode_lens.append(ep_len[env_ind_local])
                        episode_rews.append(ep_rew[env_ind_local])
                        episode_start_indices.append(ep_idx[env_ind_local])
                        # now we copy obs_next to obs, but since there might be
                        # finished episodes, we have to reset finished envs first.
                        _reset_env_with_ids(
                            env_ind_local
                        )
                        # for i in env_ind_local:
                            # _reset_state(i)
                    # for agent in learning_agents:
                    #     whole_con_data[agent].obs = con_data[agent].obs_next
                    #     whole_con_data[agent].rew = con_data[agent].rew
                    #     whole_con_data[agent].done = con_data[agent].done
                    #     whole_con_data[agent].info = con_data[agent].info
                    #     con_data[agent] = whole_con_data[agent]
                    #     whole_dis_data[agent].obs = dis_data[agent].obs_next
                    #     whole_dis_data[agent].rew = dis_data[agent].rew
                    #     whole_dis_data[agent].done = dis_data[agent].done
                    #     whole_dis_data[agent].info = dis_data[agent].info
                    #     dis_data[agent] = whole_dis_data[agent]
                    #     print(con_data[agent].act)
                    if (args.step_per_epoch and step_count >= args.step_per_epoch):
                        break
                # 处理并学习
                _ready_env_ids = ready_env_ids
                collect_step += step_count
                collect_episode += episode_count
                collect_time += max(time.time() - start_time, 1e-9)
                if episode_count > 0:
                    rews, lens, idxs = list(
                        map(
                            np.concatenate,
                            [episode_rews, episode_lens, episode_start_indices]
                        )
                    )
                    rew_mean, rew_std = rews.mean(), rews.std()
                    len_mean, len_std = lens.mean(), lens.std()
                else:
                    rews, lens, idxs = np.array([]), np.array([], int), np.array([], int)
                    rew_mean = rew_std = len_mean = len_std = 0
                result = {
                    "n/ep": episode_count,
                    "n/st": step_count,
                    "rews": rews,
                    "lens": lens,
                    "idxs": idxs,
                    "rew": rew_mean,
                    "len": len_mean,
                    "rew_std": rew_std,
                    "len_std": len_std,
                }
                if result["n/ep"] > 0 and reward_metric:
                    rew = reward_metric(result["rews"])
                    result.update(rews=rew, rew=rew.mean(), rew_std=rew.std())
                last_rew = result["rew"] if result["n/ep"] > 0 else last_rew
                last_len = result["len"] if result["n/ep"] > 0 else last_len
                data = {
                    "rew": f"{last_rew:.2f}",
                    "len": str(int(last_len)),
                    "n/ep": str(int(result["n/ep"])),
                    "n/st": str(int(result["n/st"])),
                }

                # test in train?
                t.update(result["n/st"])
                all_rew.append(np.array(temp_rew).mean())
        all_con_losses: Dict[str, Any] = dict()
        all_dis_losses: Dict[str, Any] = dict()
        for agent in learning_agents:
            all_con_losses[agent] = con_losses
            dis_losses = d_policies[agent].update(
                0,
                d_buffers[agent],
                batch_size=args.batch_size,
                repeat=args.repeat_per_collect,
            )
            d_buffers[agent].reset()
            all_dis_losses[agent] = dis_losses
        print("done")
    print(all_rew)
    plt.plot(all_rew)
    plt.show()






if __name__ == '__main__':
    test_decmappo()
